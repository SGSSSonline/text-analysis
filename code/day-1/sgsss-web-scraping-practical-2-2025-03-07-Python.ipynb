{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SGSSS Logo](../../img/SGSSS_Stacked.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Practical Computational Methods for Social Scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Computational methods are transforming research practice across the disciplines. For social scientists these methods offer a number of valuable opportunities, including creating new datasets from digital sources; unearthing new insights and avenues for research from existing data sources; and improving the accuracy and efficiency of fundamental research activities.\n",
    "\n",
    "In this lesson we apply the logic of web scraping to some a simple, genuine website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aims\n",
    "\n",
    "This lesson has two aims:\n",
    "1. Demonstrate how to use Python to collect data found on more complex websites.\n",
    "2. Cultivate your computational thinking skills through coding examples. In particular, how to define and solve a data collection problem using a computational method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson details\n",
    "\n",
    "* **Level**: Introductory\n",
    "* **Time**: 40-60 minutes\n",
    "* **Pre-requisites**: None\n",
    "* **Audience**: Researchers and analysts from any disciplinary background\n",
    "* **Learning outcomes**:\n",
    "    1. Understand the key steps and requirements for collecting data from web pages using computational methods.\n",
    "    2. Be able to use Python for requesting, parsing, extracting and saving data stored on a web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Guide to using this resource\n",
    "\n",
    "This learning resource was built using <a href=\"https://jupyter.org/\" target=_blank>Jupyter Notebook</a>, an open-source software application that allows you to mix code, results and narrative in a single document. As <a href=\"https://jupyter4edu.github.io/jupyter-edu-book/\" target=_blank>Barba et al. (2019)</a> espouse:\n",
    "> In a world where every subject matter can have a data-supported treatment, where computational devices are omnipresent and pervasive, the union of natural language and computation creates compelling communication and learning opportunities.\n",
    "\n",
    "If you are familiar with Jupyter notebooks then skip ahead to the main content (*What is the general approach for scraping data from a web page?*). Otherwise, the following is a quick guide to navigating and interacting with the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interaction\n",
    "\n",
    "**You only need to execute the code that is contained in sections which are marked by `In []`.**\n",
    "\n",
    "To execute a cell, click or double-click the cell and press the `Run` button on the top toolbar (you can also use the keyboard shortcut Shift + Enter).\n",
    "\n",
    "Try it for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Enter your name and press enter:\")\n",
    "name = input()\n",
    "print(\"\\r\")\n",
    "print(\"Hello {}, enjoy learning more about Python and web-scraping!\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learn more\n",
    "\n",
    "Jupyter notebooks provide rich, flexible features for conducting and documenting your data analysis workflow. To learn more about additional notebook features, we recommend working through some of the <a href=\"https://github.com/darribas/gds19/blob/master/content/labs/lab_00.ipynb\" target=_blank>materials</a> provided by Dani Arribas-Bel at the University of Liverpool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the general approach for scraping data from a web page?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We begin by identifying a web page containing information we are interested in collecting. Then we need to **know** the following:\n",
    "* The location (i.e., web address) where the web page can be accessed. For example, the SGSSS homepage can be accessed via <a href=\"https://www.sgsss.ac.uk/\" target=_blank>https://www.sgsss.ac.uk/</a>.\n",
    "* The location of the information we are interested in within the structure of the web page. This involves visually inspecting a web page's underlying code using a web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And **do** the following:\n",
    "* Request the web page using its web address.\n",
    "* Parse the structure of the web page so your programming language can work with its contents.\n",
    "* Extract the information we are interested in.\n",
    "* Write this information to a file for future use.\n",
    "\n",
    "For any programming task, it is useful to write out the steps needed to solve the problem: we call this *pseudo-code*, as it is captures the main tasks and the order in which they need to be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A more complex web scraping example\n",
    "\n",
    "Let's work through the steps in our general approach using a real web page, one that is **not** designed for practicing web scraping but instead contains information relevant to a social science research project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Identifying the web address\n",
    "\n",
    "The web page we are interested in can be found at the following web address: <a href=\"https://www.edinburgh.gov.uk/directory/10258/other-warm-and-welcoming-locations\" target=_blank>https://www.edinburgh.gov.uk/directory/10258/other-warm-and-welcoming-locations</a>.\n",
    "\n",
    "This is a web page on City of Edinburgh Council's website containing a list of organisations that provide warm and welcoming spaces to residents. There is no data file to download, therefore the only way to get this information is by scraping the web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Locating information\n",
    "\n",
    "Our task is to extract the list of organisations providing warm and welcoming spaces. In order to do so, we need to understand where the text is located within the underlying *source code* of the web page. Web pages are written in a langauge called HyperText Markup Language (HTML). HTML describes the structure of a web page, and consists of a number of elements (e.g., paragraphs, tables, headers), with each element represented by a tag (e.g., `<p>`, `<table>`, `<h1>`). Browsers do not display the HTML tags, but use them to render the content of the page.\n",
    "\n",
    "See <a href=\"https://www.w3schools.com/html/html_intro.asp\" target=_blank>https://www.w3schools.com/html/html_intro.asp</a> for more information on HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Visually inspecting the underlying HTML code\n",
    "\n",
    "Therefore, what we need are the tags that identify the section of the web page where the text is stored. We can discover the tags by examining the *source code* (HTML) of the web page. This can be done using your web browser: for example, if you use use Firefox you can right-click on the web page and select *View Page Source* from the list of options. (Chrome: *View page source*; Safari: follow <a href=\"https://www.lifewire.com/view-html-source-in-safari-3469315\" target=_blank>these instructions</a>).\n",
    "\n",
    "The cell below shows a sample of the HTML code for the web page."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "                                                                                                      <h1 class=\"page-heading\">\n",
    "    Other warm and welcoming locations\n",
    "</h1>                                        \n",
    "                                                                                    <div class=\"page-content\">\n",
    "                        <h2 class=\"page-subheading\">Information on Other warm and welcoming locations</h2>\n",
    "    \n",
    "    <ul class=\"list list--navigation list--2up\"><li class=\"list__item\"><a class=\"list__link\" href=\"/directory/10258/other-warm-and-welcoming-locations/category/10512\">Charitable organisation</a></li><li class=\"list__item\"><a class=\"list__link\" href=\"/directory/10258/other-warm-and-welcoming-locations/category/10507\">Community centres and organisations</a></li><li class=\"list__item\"><a class=\"list__link\" href=\"/directory/10258/other-warm-and-welcoming-locations/category/10511\">Culture and museums</a></li><li class=\"list__item\"><a class=\"list__link\" href=\"/directory/10258/other-warm-and-welcoming-locations/category/10509\">Place of worship</a></li></ul>    \n",
    "                                                                                            \n",
    "\n",
    "                <div class=\"boxed\">\n",
    "\n",
    "                    <form class=\"form boxed__form\" action=\"/directory/search\" enctype=\"multipart/form-data\">\n",
    "                <fieldset>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our data collection task is a little more complicated that the previous lesson, as the information we require is not actually on this page, instead we have links to multiple pages that might contain the information we require."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup Python with the modules it needs, as well as the file paths and dates necessary to process the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import string # module for working with ASCII and other strings\n",
    "import os # module for navigating your machine (e.g., file directories)\n",
    "import requests # module for requesting urls\n",
    "import json # module for working with JSON data structures\n",
    "import csv # module for working with csv files\n",
    "import pandas as pd  # module for working with dataframes\n",
    "from datetime import datetime as dt # module for working with dates and time\n",
    "from bs4 import BeautifulSoup as soup # module for parsing web pages\n",
    "\n",
    "print(\"Succesfully imported necessary modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "\n",
    "other_data = \"./data/\"\n",
    "la_data = \"./data/local-authorities/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create data folders\n",
    "\n",
    "for folder in other_data, la_data:\n",
    "    try:\n",
    "        os.mkdir(folder)\n",
    "    except:\n",
    "        print(\"Unable to create {}: already exists\".format(folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** What do you think the `for loop` and the `try-except` clause are doing in the code block above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download date\n",
    "\n",
    "ddate = dt.now().strftime(\"%Y-%m-%d\")\n",
    "ddate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Requesting the web page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now, let's implement the process of scraping the page. First, we need to request the web page using Python; this is analogous to opening a web browser and entering the web address manually. We refer to a page's location on the internet as its web address or Uniform Resource Locator (URL).\n",
    "\n",
    "The specific page we need is https://www.edinburgh.gov.uk/directory/10258/a-to-z/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\"}\n",
    "base = \"https://www.edinburgh.gov.uk/directory/10258/a-to-z/\"\n",
    "abc = list(string.ascii_uppercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack the above code as we haven't seen it before. The first line creates a variable called `header` which provides information on the types of web browsers used to request web pages. This is an important step as most modern websites are able to detect whether a URL has been requested using programming code and subsequently refuse to accept the request. The information in the `header` variable essentially tells the website you are making request using a web browser - this could be considered deceptive but it is a technical barrier, not an ethical one (we will discuss those ones in the lecture).\n",
    "\n",
    "The second line defines a base / stem of a URL - that is, it is not a complete URL but can be if the right information is appended to the end of it. For example *https://www.edinburgh.gov.uk/directory/10258/a-to-z/A* is a valid URL (the web page containing the list of organisations whose name begins with the letter 'A'). We define a base URL because we have many web pages to loop over.\n",
    "\n",
    "The final line creates a variable called `abc` that contains a list of the letters in the English alphabet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Print the contents of the `abc` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK let's start requesting the list of organisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a variable to store the list of organisations\n",
    "\n",
    "org_list = []\n",
    "\n",
    "# Loop over the A-Z list of organisations\n",
    "    \n",
    "for l in abc:\n",
    "    url = base + str(l) # build the URL to be requested\n",
    "    response = requests.get(url, headers=header) # request the web address\n",
    "    \n",
    "    if response.status_code==200: # if the web page is successfully requested\n",
    "        orgs = soup(response.text, \"html.parser\")\n",
    "        try:\n",
    "            results = orgs.find(\"ul\", class_=\"list list--record\").find_all(\"li\") # find all organisations listed on page\n",
    "            for el in results:\n",
    "                name = el.find(\"a\").text # extract organisation name from <a> tag\n",
    "                link = el.find(\"a\").get(\"href\") # extract organisation URL from <a> tag\n",
    "                obs = {\"org_name\": name, \"org_url\": link} # create an observation (dictionary) with details of each organisations\n",
    "                #print(obs)\n",
    "                org_list.append(obs)\n",
    "        except:\n",
    "             print(\"Could not find list of organisations for letter {}\".format(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**TASK**: Write pseudo-code that explains the logic and steps in the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Check the web pages that did not provide information on organisations. Are there genuinely no organisations listed or is the code failing to identify them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The above code should produce a list of observations containing details for each organisation listed under each web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(org_list)) # display number of elements in list\n",
    "org_list # display contents of list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Excellent, we have a list of organisations that provide warm and welcoming spaces. Now it's time to go to each organisation's web page on the Council's website and extract the information of interest. See an example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"https://www.edinburgh.gov.uk/directory-record/1697699/action-porty-\", width=\"1000\", height=\"650\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh, we cannot request and display the web page through this notebook. Most modern websites have protections in place that prevent their contents being displayed elsewhere - otherwise it would be possible to display a website under a different URL / location and claim it as your own. This isn't an issue for web scraping but simply an example of how real, functioning websites provide additional challenges when collecting data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the source code of the web page using our browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the web page and extracting information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's speed up the process by requesting each organisation's web page, parsing it as HTML and extracting the information required in a single block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "org_details = []\n",
    "base = \"https://www.edinburgh.gov.uk\"\n",
    "\n",
    "for org in org_list:\n",
    "    url = base + org[\"org_url\"]\n",
    "    \n",
    "    response = requests.get(url, headers=header) # request the web address\n",
    "    if response.status_code==200:\n",
    "        soup_org = soup(response.text, \"html.parser\")\n",
    "        results = soup_org.find(\"dl\", class_=\"list list--definition definition\")\n",
    "        #print(results)\n",
    "        \n",
    "        dts = results.find_all(\"dt\")\n",
    "        dt_list = []\n",
    "        for dt in dts:\n",
    "            dt_list.append(dt.text.strip())\n",
    "            \n",
    "        dds = results.find_all(\"dd\")\n",
    "        dd_list = []\n",
    "        for dd in dds:\n",
    "            dd_list.append(dd.text.strip())\n",
    "        \n",
    "        obs = dict(zip(dt_list, dd_list))\n",
    "        obs[\"org_name\"] = org[\"org_name\"]\n",
    "        obs[\"org_url\"] = url\n",
    "        #print(obs)\n",
    "        \n",
    "        org_details.append(obs)\n",
    "    else:\n",
    "        print(\"Could not request webpage for organisation {}\".format(org[\"org_name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_details[0:3] # display the first three elements in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Saving results from the scrape\n",
    "\n",
    "Let's conclude by saving the scraped data to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outfile = la_data + \"coe-warm-spaces-\" + ddate + \".json\"\n",
    "with open(outfile, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(org_details, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we know this worked? The simplest way is to check whether a) the file was created, and b) the results were written to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Check presence of file in current folder\n",
    "\n",
    "os.listdir(\"./data/local-authorities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Open file and read (import) its contents\n",
    "\n",
    "with open(outfile, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "print(data)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And Voila, we have successfully scraped multiple web pages!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What have we learned?\n",
    "\n",
    "Let's recap what key skills and techniques we've learned:\n",
    "* **How to import modules**. You will usually need to import modules into Python to support your work. Python does come with some methods and functions that are ready to use straight away, but for computational social science tasks you'll almost certainly need to import some additional modules.\n",
    "* **How to request and parse web pages**. You can use Python to request a web page, and the `BeautifulSoup` module to parse its contents.\n",
    "* **How to read and write data**. You can save the results of your scrape to a file for future use.\n",
    "* **How to do all of the above in an efficient, clear and effective manner**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "The above examples demonstrate the basics of using computational methods for collecting social science data from real websites. These techniques should be sufficient for most social science-relevant data collection exercises, however there are more complicated examples:\n",
    "* Data may only appear on a web page after user input (e.g., https://www.charities.gov.sg/Pages/AdvanceSearch.aspx).\n",
    "* Data may be contained in embedded maps and thus are more difficult to extract (e.g., https://www.foodaidnetwork.org.uk/our-members).\n",
    "* And many other potential issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to our example from City of Edinburgh Council, see if you can scrape the list of public libraries:\n",
    "* https://www.edinburgh.gov.uk/directory/10199/library-locations-and-opening-hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is provided at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\"}\n",
    "base = \"https://www.edinburgh.gov.uk/directory/10199/a-to-z/\"\n",
    "abc = list(string.ascii_uppercase)\n",
    "\n",
    "org_list = []\n",
    "    \n",
    "for l in abc:\n",
    "    url = base + str(l)\n",
    "    print(url)\n",
    "    response = requests.get(url, headers=header) # request the web address\n",
    "    \n",
    "    if response.status_code==200:\n",
    "        orgs = soup(response.text, \"html.parser\")\n",
    "        try:\n",
    "            results = orgs.find(\"ul\", class_=\"list list--record\").find_all(\"li\")\n",
    "            for el in results:\n",
    "                name = el.find(\"a\").text\n",
    "                link = el.find(\"a\").get(\"href\")\n",
    "                obs = {\"org_name\": name, \"org_url\": link}\n",
    "                #print(obs)\n",
    "                org_list.append(obs)\n",
    "        except:\n",
    "             print(\"Could not find list of organisations\")\n",
    "            \n",
    "#print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_details = []\n",
    "base = \"https://www.edinburgh.gov.uk\"\n",
    "for org in org_list:\n",
    "    url = base + org[\"org_url\"]\n",
    "    \n",
    "    response = requests.get(url, headers=header) # request the web address\n",
    "    if response.status_code==200:\n",
    "        soup_org = soup(response.text, \"html.parser\")\n",
    "        results = soup_org.find(\"dl\", class_=\"list list--definition definition\")\n",
    "        #print(results)\n",
    "        \n",
    "        dts = results.find_all(\"dt\")\n",
    "        dt_list = []\n",
    "        for dt in dts:\n",
    "            dt_list.append(dt.text.strip())\n",
    "            \n",
    "        dds = results.find_all(\"dd\")\n",
    "        dd_list = []\n",
    "        for dd in dds:\n",
    "            dd_list.append(dd.text.strip())\n",
    "        \n",
    "        obs = dict(zip(dt_list, dd_list))\n",
    "        obs[\"org_name\"] = org[\"org_name\"]\n",
    "        obs[\"org_url\"] = url\n",
    "        #print(obs)\n",
    "        \n",
    "        org_details.append(obs)\n",
    "    else:\n",
    "        print(\"Could not request webpage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_details[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--END OF FILE--"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "256px",
    "width": "221px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
