{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SGSSS Logo](../../img/SGSSS_Stacked.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Practical Computational Methods for Social Scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Computational methods are transforming research practice across the disciplines. For social scientists these methods offer a number of valuable opportunities, including creating new datasets from digital sources; unearthing new insights and avenues for research from existing data sources; and improving the accuracy and efficiency of fundamental research activities.\n",
    "\n",
    "In this lesson we apply the logic of web scraping to a simple, genuine website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aims\n",
    "\n",
    "This lesson has two aims:\n",
    "1. Demonstrate how to use R to collect data found on more complex websites.\n",
    "2. Cultivate your computational thinking skills through coding examples. In particular, how to define and solve a data collection problem using a computational method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson details\n",
    "\n",
    "* **Level**: Introductory\n",
    "* **Time**: 40-60 minutes\n",
    "* **Pre-requisites**: None\n",
    "* **Audience**: Researchers and analysts from any disciplinary background\n",
    "* **Learning outcomes**:\n",
    "    1. Understand the key steps and requirements for collecting data from web pages using computational methods.\n",
    "    2. Be able to use R for requesting, parsing, extracting and saving data stored on a web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Guide to using this resource\n",
    "\n",
    "This learning resource was built using <a href=\"https://jupyter.org/\" target=_blank>Jupyter Notebook</a>, an open-source software application that allows you to mix code, results and narrative in a single document. As <a href=\"https://jupyter4edu.github.io/jupyter-edu-book/\" target=_blank>Barba et al. (2019)</a> espouse:\n",
    "> In a world where every subject matter can have a data-supported treatment, where computational devices are omnipresent and pervasive, the union of natural language and computation creates compelling communication and learning opportunities.\n",
    "\n",
    "If you are familiar with Jupyter notebooks then skip ahead to the main content (*What is the general approach for scraping data from a web page?*). Otherwise, the following is a quick guide to navigating and interacting with the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interaction\n",
    "\n",
    "**You only need to execute the code that is contained in sections which are marked by `In []`.**\n",
    "\n",
    "To execute a cell, click or double-click the cell and press the `Run` button on the top toolbar (you can also use the keyboard shortcut Shift + Enter).\n",
    "\n",
    "Try it for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "name <- readline(prompt=\"Enter name: \")\n",
    "print(paste(\"Hi,\", name, \"enjoy learning more about R and web-scraping!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learn more\n",
    "\n",
    "Jupyter notebooks provide rich, flexible features for conducting and documenting your data analysis workflow. To learn more about additional notebook features, we recommend working through some of the <a href=\"https://github.com/darribas/gds19/blob/master/content/labs/lab_00.ipynb\" target=_blank>materials</a> provided by Dani Arribas-Bel at the University of Liverpool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the general approach for scraping data from a web page?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We begin by identifying a web page containing information we are interested in collecting. Then we need to **know** the following:\n",
    "* The location (i.e., web address) where the web page can be accessed. For example, the SGSSS homepage can be accessed via <a href=\"https://www.sgsss.ac.uk/\" target=_blank>https://www.sgsss.ac.uk/</a>.\n",
    "* The location of the information we are interested in within the structure of the web page. This involves visually inspecting a web page's underlying code using a web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And **do** the following:\n",
    "* Request the web page using its web address.\n",
    "* Parse the structure of the web page so your programming language can work with its contents.\n",
    "* Extract the information we are interested in.\n",
    "* Write this information to a file for future use.\n",
    "\n",
    "For any programming task, it is useful to write out the steps needed to solve the problem: we call this *pseudo-code*, as it is captures the main tasks and the order in which they need to be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A more complex web scraping example\n",
    "\n",
    "Let's work through the steps in our general approach using a real web page, one that is **not** designed for practicing web scraping but instead contains information relevant to a social science research project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Identifying the web address\n",
    "\n",
    "The web page we are interested in can be found at the following web address: <a href=\"https://www.edinburgh.gov.uk/directory/10258/other-warm-and-welcoming-locations\" target=_blank>https://www.edinburgh.gov.uk/directory/10258/other-warm-and-welcoming-locations</a>.\n",
    "\n",
    "This is a web page on City of Edinburgh Council's website containing a list of organisations that provide warm and welcoming spaces to residents. There is no data file to download, therefore the only way to get this information is by scraping the web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Locating information\n",
    "\n",
    "Our task is to extract the list of organisations providing warm and welcoming spaces. In order to do so, we need to understand where the text is located within the underlying *source code* of the web page. Web pages are written in a langauge called HyperText Markup Language (HTML). HTML describes the structure of a web page, and consists of a number of elements (e.g., paragraphs, tables, headers), with each element represented by a tag (e.g., `<p>`, `<table>`, `<h1>`). Browsers do not display the HTML tags, but use them to render the content of the page.\n",
    "\n",
    "See <a href=\"https://www.w3schools.com/html/html_intro.asp\" target=_blank>https://www.w3schools.com/html/html_intro.asp</a> for more information on HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Visually inspecting the underlying HTML code\n",
    "\n",
    "Therefore, what we need are the tags that identify the section of the web page where the text is stored. We can discover the tags by examining the *source code* (HTML) of the web page. This can be done using your web browser: for example, if you use use Firefox you can right-click on the web page and select *View Page Source* from the list of options. (Chrome: *View page source*; Safari: follow <a href=\"https://www.lifewire.com/view-html-source-in-safari-3469315\" target=_blank>these instructions</a>).\n",
    "\n",
    "The cell below shows a sample of the HTML code for the web page."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "                                                                                                      <h1 class=\"page-heading\">\n",
    "    Other warm and welcoming locations\n",
    "</h1>                                        \n",
    "                                                                                    <div class=\"page-content\">\n",
    "                        <h2 class=\"page-subheading\">Information on Other warm and welcoming locations</h2>\n",
    "    \n",
    "    <ul class=\"list list--navigation list--2up\"><li class=\"list__item\"><a class=\"list__link\" href=\"/directory/10258/other-warm-and-welcoming-locations/category/10512\">Charitable organisation</a></li><li class=\"list__item\"><a class=\"list__link\" href=\"/directory/10258/other-warm-and-welcoming-locations/category/10507\">Community centres and organisations</a></li><li class=\"list__item\"><a class=\"list__link\" href=\"/directory/10258/other-warm-and-welcoming-locations/category/10511\">Culture and museums</a></li><li class=\"list__item\"><a class=\"list__link\" href=\"/directory/10258/other-warm-and-welcoming-locations/category/10509\">Place of worship</a></li></ul>    \n",
    "                                                                                            \n",
    "\n",
    "                <div class=\"boxed\">\n",
    "\n",
    "                    <form class=\"form boxed__form\" action=\"/directory/search\" enctype=\"multipart/form-data\">\n",
    "                <fieldset>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our data collection task is a little more complicated that the previous lesson, as the information we require is not actually on this page, instead we have links to multiple pages that might contain the information we require."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup R with the libraries it needs, as well as the file paths and dates necessary to process the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "\n",
    "library(stringr)  # For string manipulation\n",
    "library(fs)  # For file system operations\n",
    "library(httr)  # For HTTP requests\n",
    "library(jsonlite)  # For JSON manipulation\n",
    "library(readr)  # For reading and writing CSV files\n",
    "library(dplyr)  # For data manipulation\n",
    "library(lubridate)  # For dates and times\n",
    "library(rvest)  # For web scraping\n",
    "\n",
    "cat(\"Successfully imported necessary libraries\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "\n",
    "other_data <- \"./data/\"\n",
    "la_data <- \"./data/local-authorities/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create data folders\n",
    "\n",
    "for (folder in c(other_data, la_data)) {\n",
    "  if (!dir.exists(folder)) {\n",
    "    dir_create(folder)\n",
    "  } else {\n",
    "    cat(sprintf(\"Unable to create %s: already exists\\n\", folder))\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** What do you think the `for loop` and the `if-else` clause are doing in the code block above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download date\n",
    "\n",
    "ddate <- format(Sys.Date(), \"%Y-%m-%d\")\n",
    "cat(ddate, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Requesting the web page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now, let's implement the process of scraping the page. First, we need to request the web page using Python; this is analogous to opening a web browser and entering the web address manually. We refer to a page's location on the internet as its web address or Uniform Resource Locator (URL).\n",
    "\n",
    "The specific page we need is https://www.edinburgh.gov.uk/directory/10258/a-to-z/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header correctly using add_headers()\n",
    "header <- c(\n",
    "  \"User-Agent\" = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\"\n",
    ")\n",
    "base <- \"https://www.edinburgh.gov.uk/directory/10258/a-to-z/\"\n",
    "abc <- LETTERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack the above code as we haven't seen it before. The first line creates a variable called `header` which provides information on the types of web browsers used to request web pages. This is an important step as most modern websites are able to detect whether a URL has been requested using programming code and subsequently refuse to accept the request. The information in the `header` variable essentially tells the website you are making request using a web browser - this could be considered deceptive but it is a technical barrier, not an ethical one (we will discuss those ones in the lecture).\n",
    "\n",
    "The second line defines a base / stem of a URL - that is, it is not a complete URL but can be if the right information is appended to the end of it. For example *https://www.edinburgh.gov.uk/directory/10258/a-to-z/A* is a valid URL (the web page containing the list of organisations whose name begins with the letter 'A'). We define a base URL because we have many web pages to loop over.\n",
    "\n",
    "The final line creates a variable called `abc` that contains a list of the letters in the English alphabet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** Print the contents of the `abc` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK let's start requesting the list of organisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a variable to store the list of organisations\n",
    "\n",
    "org_list <- list()\n",
    "\n",
    "# Loop over the A-Z list of organisations\n",
    "    \n",
    "for (l in abc) {\n",
    "  url <- paste0(base, l)  # Build the URL\n",
    "  response <- GET(url, add_headers(.headers = header))  # Request the web address\n",
    "  \n",
    "  if (status_code(response) == 200) {  # If the web page is successfully requested\n",
    "    orgs <- read_html(content(response, as = \"text\"))\n",
    "    tryCatch({\n",
    "      results <- html_elements(orgs, \"ul.list.list--record li\")  # Find all organizations listed\n",
    "      for (el in results) {\n",
    "        name <- html_text(html_element(el, \"a\"))  # Extract organization name\n",
    "        link <- html_attr(html_element(el, \"a\"), \"href\")  # Extract URL\n",
    "        obs <- list(org_name = name, org_url = link)\n",
    "        org_list <- append(org_list, list(obs))\n",
    "      }\n",
    "    }, error = function(e) {\n",
    "      cat(sprintf(\"Could not find list of organizations for letter %s\\n\", l))\n",
    "    })\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**TASK**: Write pseudo-code that explains the logic and steps in the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Check the web pages that did not provide information on organisations. Are there genuinely no organisations listed or is the code failing to identify them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The above code should produce a list of observations containing details for each organisation listed under each web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(length(org_list), \"\\n\")\n",
    "print(org_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Excellent, we have a list of organisations that provide warm and welcoming spaces. Now it's time to go to each organisation's web page on the Council's website and extract the information of interest. See an example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(IRdisplay)\n",
    "display_html('<iframe src=\"https://www.edinburgh.gov.uk/directory-record/1697699/action-porty-\" width=\"1000\" height=\"650\"></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh, we cannot request and display the web page through this notebook. Most modern websites have protections in place that prevent their contents being displayed elsewhere - otherwise it would be possible to display a website under a different URL / location and claim it as your own. This isn't an issue for web scraping but simply an example of how real, functioning websites provide additional challenges when collecting data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the source code of the web page using our browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the web page and extracting information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's speed up the process by requesting each organisation's web page, parsing it as HTML and extracting the information required in a single block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "org_details <- list()\n",
    "base <- \"https://www.edinburgh.gov.uk\"\n",
    "\n",
    "for (org in org_list) {\n",
    "  url <- paste0(base, org$org_url)\n",
    "  response <- GET(url, add_headers(.headers = header))\n",
    "  \n",
    "  if (status_code(response) == 200) {\n",
    "    soup_org <- read_html(content(response, as = \"text\"))\n",
    "    results <- html_element(soup_org, \"dl.list.list--definition.definition\")\n",
    "    \n",
    "    dt_list <- html_text(html_elements(results, \"dt\"))\n",
    "    dd_list <- html_text(html_elements(results, \"dd\"))\n",
    "    \n",
    "    obs <- as.list(setNames(dd_list, dt_list))\n",
    "    obs$org_name <- org$org_name\n",
    "    obs$org_url <- url\n",
    "    org_details <- append(org_details, list(obs))\n",
    "  } else {\n",
    "    cat(sprintf(\"Could not request webpage for organization %s\\n\", org$org_name))\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_details[1:3] # display the first three elements in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Saving results from the scrape\n",
    "\n",
    "Let's conclude by saving the scraped data to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outfile <- paste0(la_data, \"coe-warm-spaces-\", ddate, \".json\")\n",
    "write_json(org_details, outfile, pretty = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we know this worked? The simplest way is to check whether a) the file was created, and b) the results were written to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Check presence of file in current folder\n",
    "\n",
    "list.files(\"./data/local-authorities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Open file and read (import) its contents\n",
    "\n",
    "data <- fromJSON(outfile)\n",
    "print(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And Voila, we have successfully scraped multiple web pages!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What have we learned?\n",
    "\n",
    "Let's recap what key skills and techniques we've learned:\n",
    "* **How to import modules**. You will usually need to import libraries into R to support your work. R does come with some methods and functions that are ready to use straight away, but for computational social science tasks you'll almost certainly need to import some additional modules.\n",
    "* **How to request and parse web pages**. You can use R to request a web page, and the `rvest` module to parse its contents.\n",
    "* **How to read and write data**. You can save the results of your scrape to a file for future use.\n",
    "* **How to do all of the above in an efficient, clear and effective manner**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "The above examples demonstrate the basics of using computational methods for collecting social science data from real websites. These techniques should be sufficient for most social science-relevant data collection exercises, however there are more complicated examples:\n",
    "* Data may only appear on a web page after user input (e.g., https://www.charities.gov.sg/Pages/AdvanceSearch.aspx).\n",
    "* Data may be contained in embedded maps and thus are more difficult to extract (e.g., https://www.foodaidnetwork.org.uk/our-members).\n",
    "* And many other potential issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to our example from City of Edinburgh Council, see if you can scrape the list of public libraries:\n",
    "* https://www.edinburgh.gov.uk/directory/10199/library-locations-and-opening-hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is provided at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header as a character vector\n",
    "header <- c(\n",
    "  \"User-Agent\" = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\"\n",
    ")\n",
    "\n",
    "# Define base URL and alphabet list\n",
    "base <- \"https://www.edinburgh.gov.uk/directory/10199/a-to-z/\"\n",
    "abc <- LETTERS  # Equivalent to string.ascii_uppercase in Python\n",
    "\n",
    "# Initialize empty list for storing organization details\n",
    "org_list <- list()\n",
    "\n",
    "# Loop over each letter in the alphabet\n",
    "for (l in abc) {\n",
    "  url <- paste0(base, l)  # Construct the URL\n",
    "  cat(url, \"\\n\")  # Print URL to console\n",
    "  \n",
    "  # Make the GET request with custom headers\n",
    "  response <- GET(url, add_headers(.headers = header))\n",
    "  \n",
    "  # Check if request was successful\n",
    "  if (status_code(response) == 200) {\n",
    "    # Parse the HTML content\n",
    "    orgs <- read_html(content(response, as = \"text\"))\n",
    "    \n",
    "    # Try to find and extract the relevant information\n",
    "    tryCatch({\n",
    "      # Find all <li> elements within the <ul> with class \"list list--record\"\n",
    "      results <- html_elements(orgs, \"ul.list.list--record li\")\n",
    "      \n",
    "      # Loop through each result\n",
    "      for (el in results) {\n",
    "        # Extract organization name and URL\n",
    "        name <- html_text(html_element(el, \"a\"))\n",
    "        link <- html_attr(html_element(el, \"a\"), \"href\")\n",
    "        \n",
    "        # Store the extracted information in a named list\n",
    "        obs <- list(org_name = name, org_url = link)\n",
    "        \n",
    "        # Append the list to org_list\n",
    "        org_list <- append(org_list, list(obs))\n",
    "      }\n",
    "    }, error = function(e) {\n",
    "      cat(\"Could not find list of organizations\\n\")\n",
    "    })\n",
    "  }\n",
    "}\n",
    "\n",
    "# Display the first few elements of the result\n",
    "head(org_list, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base URL\n",
    "base <- \"https://www.edinburgh.gov.uk\"\n",
    "\n",
    "# Initialize an empty list to store organization details\n",
    "org_details <- list()\n",
    "\n",
    "# Loop over each organization in org_list\n",
    "for (org in org_list) {\n",
    "  url <- paste0(base, org$org_url)  # Construct the full URL\n",
    "  \n",
    "  # Make the GET request with custom headers\n",
    "  response <- GET(url, add_headers(.headers = header))\n",
    "  \n",
    "  # Check if the request was successful\n",
    "  if (status_code(response) == 200) {\n",
    "    # Parse the HTML content\n",
    "    soup_org <- read_html(content(response, as = \"text\"))\n",
    "    \n",
    "    # Find the relevant section with class \"list list--definition definition\"\n",
    "    results <- html_element(soup_org, \"dl.list.list--definition.definition\")\n",
    "    \n",
    "    # Extract <dt> and <dd> elements\n",
    "    dt_list <- html_text(html_elements(results, \"dt\"))\n",
    "    dd_list <- html_text(html_elements(results, \"dd\"))\n",
    "    \n",
    "    # Combine into a named list (similar to Python's zip function)\n",
    "    obs <- as.list(setNames(dd_list, dt_list))\n",
    "    obs$org_name <- org$org_name\n",
    "    obs$org_url <- url\n",
    "    \n",
    "    # Append the observation to org_details\n",
    "    org_details <- append(org_details, list(obs))\n",
    "  } else {\n",
    "    cat(\"Could not request webpage\\n\")\n",
    "  }\n",
    "}\n",
    "\n",
    "# Display the first three elements of org_details\n",
    "print(head(org_details, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--END OF FILE--"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "256px",
    "width": "221px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
